# eBPF使用场景

## 大面儿

了解什么时候能用（以及什么时候不能用）。

## 什么是eBPF？

eBPF是Linux内核中的一个特性，可以让你在内核中运行一个虚拟机。这个虚拟机可以让你安全地在内核中加载程序，并且定制程序。那么它为什么这么有用呢？

在老老年间，对内核进行修改是非常难的：有API可以用于获取数据，但是你不能干预内核中存在的东西或者进去执行代码。因此，你就得给Linux社区提交一个patch，等待它的通过。有了eBPF后，你可以将一个程序加载到内核中，然后比如当遇到某个特定的数据包或特定的事件发生时，命令内核执行你的程序。

有了eBPF，内核及其行为就变得高度可定制化了，而不是相对死板固定的模式。这一点及其有用，十年磨一剑，可以起到画龙点睛的作用。

## Calico和eBPF

Calico提供了eBPF数据面和标准的Linux数据面（基于iptables）。标准数据面关注的是兼容性，并且跟kube-proxy、iptables规则一同协作完成任务，而eBPF数据面关注的则是性能和延迟，以及相对于标准数据面带来的用户体验提升。

Calico不光支持标准Linux和eBPF；目前它一共支持三种数据面，包括了Windows HNS，并且计划在不久的将来引入更多的数据面。Calico可以让你选择出最适合你的模式。

如果你在Calico中启用eBPF时还留有其他的iptables规则，我们不会动它。比如你在使用连接时负载均衡的同时还想留着那些iptables。有了Calico，这就不会成为一锤子买卖——我们可以让你根据需要轻松地加载或卸载eBPF数据面，也就是说你可以在做决定前快速地先试一下。Calico可以让你按需加载eBPF，作为Kubernetes集群安全保障的又一利器。

## 使用场景

这里给出一些使用场景，包括流控、创建网络策略、连接时负载均衡。

### 流控

没有eBPF的话，数据包通过标准Linux网络路径一路到达目的地。如果一个数据包出现在A点，并且你知道它需要到达B点，你可以对这条网络路径进行优化，直接把它送到B点。有了eBPF，你可以利用额外的上下文在内核中完成这些修改，这样数据包可以绕过复杂的路由机制，直接到达目的地。

这在Kubernetes容器环境中是非常有用的，因为你的网络会非常多。（除了主机网络栈，每个容器有它自己的小网络栈。）当流量进来后，通常会路由到一个容器栈，然后必须经历一套复杂的路径，就跟它从主机网络栈过来时一样的套路。可以使用eBPF绕过这部分的路由。

### 创建网络策略

创建网络策略的时候，有两种实例可以使用eBPF：

- **eXpress数据路径（XDP）**——当一个原始数据包buffer进入到系统时，eBPF可以让你快速对其进行检查，迅速做出决策。
- **网络策略**——eBPF可以让你高效的检视一个数据包并执行网络策略，对Pod和主机都可以。

### 连接时负载均衡

在Kubernetes中对服务进行负载均衡时，需要一个端口跟服务进行通信，因此一定会出现网络地址转换（NAT）。数据包发送给一个虚拟IP，然后将虚拟IP翻译成某个Pod的目标IP；然后Pod将响应发送给虚拟IP，然后返回的数据包再进行翻译，再返回给最开始的源端。

有了eBPF，可以通过eBPF程序避免这种数据包转换，将程序加载到内核中，在连接的源头进行负载均衡。这样的话所有的NAT开销就都不存在了，因为在数据包处理过程中不再需要源地址转换（DNAT）了。

## 性能的代价

所以eBPF一定比标准Linux iptables要好？直白的讲：看情况。

如果你对iptables进行微基准测试，看看它面对大量IP（比如ipsets）时配合网络策略工作的怎么样，那么iptables在许多时候要比eBPF好。但如果你想在Linux内核中搞些事情，以便于你调整数据包在内核中的流向，eBPF则是更好的选择。标准Linux iptables是一套复杂的系统，当然有其限制，但同时它也提供了操作流量的方法；如果你了解如何对iptables规则进行编程，那么你也会受益颇丰。eBPF可以让你把程序加载到内核中按照你的需求完成它们的任务，因此它要比iptables更灵活，不仅仅是一堆规则而已。

另外需要考虑的是，虽然eBPF可以让你写程序，搞逻辑，做流控，绕过某些机制——绝对是一把利器——但它也是个虚拟机，所以一定是需要翻译成字节码。对比来看，Linux内核中的iptables则是已经编译到代码中的。

你应该也看出来了，对比eBPF和iptables不是简单的比较。我们要考量的是性能，其中两个重要的因素就是延迟（速度）和开销。如果eBPF极快但占用了你80%的资源，这样看上去有点像一辆兰博基尼——又贵又快。如果这一点你可以接受，很好（你可能就是喜欢又贵又快的）。要牢记的是CPU使用率的提升也同时要给云服务商交更多的钱。所以尽管兰博基尼很快，但如果每天上下班都有限速的话，那这笔钱花的就有点冤枉了。

## 啥时候用（啥时候不用）

有了eBPF你能得到性能提升——但也要付出一些代价。你需要找出二者之间的平衡，算好性能的代价，然后再决定是否可以接受eBPF。

让我们看一些可以用以及不可以用的场景

### ✘ 啥时候不用

### ✘ 实现应用层策略

用eBPF进行深度协议检查并实现应用层策略并不会很高效，原因就是成本和性能的权衡。你可以利用Linux内核的连接跟踪来实现策略，每个数据流只设置一次策略即可（不管这个数据流中有5个还是5000个数据包），然后再Linux的conntrack表中将其标记为放行或拒绝。你不需要检查这个流中的每个数据包。如果你用eBPF做，它可以让你再一个TCP连接中完成多个HTTP事务，但你需要检查每一个数据包，检查里面的事务并实现7层控制。要实现这些，你需要的是更多的CPU周期，代价也就更高。而更高效的做法是搞一个类似Envoy的代理，用eBPF来优化到Envoy的流量，将应用协议的翻译交给Envoy来做。这种情况下iptables加类似Envoy的代理是一种更好的设计。

### ✘ 构建服务网格控制面

同样，服务网格也依赖类似Envoy的代理。这几年这块的设计也涌现出了很多。这么做的主要原因是，在大部分情况下，在集群内对类似HTTP这种应用层协议进行高速内联处理不太现实。因此你应该考虑的是用eBPF优化到Envoy代理的流量路由，而不是直接用它来代替代理。

### ✘ 按数据包处理

用eBPF执行CPU密集型或按数据包处理的任务，比如对已加密的数据流进行解密和再加密，这样就并不高效了，因为你需要打造一种结构，检查每一个数据包，成本极高。

### ✔ 啥时候用

### ✔ XDP

eBPF可以在原始数据包buffer进入系统时进行高效的检查，让你快速制定出相关决策。

### ✔ 连接时负载均衡

有了eBPF，可以通过加载到内核中的程序进行源端负载均衡，不需要用虚拟IP。因为不需要DNAT，连接中所有的NAT开销都不复存在。

## 总结

eBPF可以替代iptables吗？不完全。很难想象iptables中能做的所有事都能高效的在eBPF中完成。目前来看它俩要共存，并且需要用户来权衡代价和性能，按需决定到底怎么用。

我们相信正确的做法应该是让eBPF跟Linux内核中已有的机制共存，方能达到你的目标。这也是为什么Calico要支持多种数据面，包括标准Linux、Windows HNS、Linux eBPF。因为我们相信eBPF和iptables都是有用的，唯一符合逻辑的做法就是两者都支持。Calico将选择权交给你，这样你就可以打造最佳的解决方案。